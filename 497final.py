# -*- coding: utf-8 -*-
"""497Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RDfwQziQzwzE8qC7XTTpygdutJ237X_9

### Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

"""### Unsloth"""

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 15 trillion tokens model 2x faster!
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # We also uploaded 4bit for 405b!
    "unsloth/Mistral-Nemo-Base-2407-bnb-4bit", # New Mistral 12b 2x faster!
    "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
    "unsloth/mistral-7b-v0.3-bnb-4bit",        # Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

"""We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

"""### Data Prep"""

# ---------- FINAL DATA-PREP CELL (run AFTER you have `tokenizer`) ----------
from google.colab import drive
drive.mount("/content/drive")

from datasets import load_dataset

# file paths
DATA_DIR = "/content/drive/MyDrive/data"
paths = {
    "train":      f"{DATA_DIR}/train.csv",
    "validation": f"{DATA_DIR}/validation.csv",
    "test":       f"{DATA_DIR}/test.csv",
    "hf_set":     f"{DATA_DIR}/humanFeedback.csv",
}

# Alpaca‐style template
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""
EOS = tokenizer.eos_token or "</s>"

def formatting_prompts_func(examples):
    texts = [
        alpaca_prompt.format(inp, ins, out) + EOS
        for ins, inp, out in zip(
            examples["instruction"],
            examples["input"],
            examples["output"],
        )
    ]
    return {"text": texts}

# 1) Load train/validation/test with CSV loader
raw = load_dataset(
    "csv",
    data_files={
        "train":      paths["train"],
        "validation": paths["validation"],
        "test":       paths["test"],
    },
    column_names=["instruction", "output"],
)

# 2) Process each split, add the fixed input, format, then slice
for split, size in [("train", 3200), ("validation", 400), ("test", 400)]:
    ds = raw[split]
    # ← fill input with the same prompt for all examples
    ds = ds.add_column(
        "input",
        ["Construct a research methodology for the given problem."] * len(ds)
    )
    ds = ds.map(
        formatting_prompts_func,
        batched=True,
        remove_columns=ds.column_names,
    )
    globals()[f"{split}_dataset"] = ds.select(range(size))

# 3) human-feedback set: likewise fill input
raw_hf = load_dataset(
    "csv",
    data_files=paths["hf_set"],
    column_names=["instruction", "output"],
    split="train",
)
raw_hf = raw_hf.add_column(
    "input",
    ["Construct a research methodology for the given problem."] * len(raw_hf)
)
hf_set = (
    raw_hf
    .map(formatting_prompts_func, batched=True, remove_columns=raw_hf.column_names)
    .select(range(10))
)

# 4) Sanity checks
print("train_dataset length:", len(train_dataset))           # → 500
print("validation_dataset length:", len(validation_dataset)) # → 50
print("test_dataset length:", len(test_dataset))             # → 50
print("hf_set length:", len(hf_set))                         # → 10

# show that Input now contains your fixed prompt
print("\nSample train text:\n", train_dataset[0]["text"])

print("\nSample test text:\n", hf_set[4]["text"])

"""<a name="Train"></a>
### Train the model
Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!
"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import FastLanguageModel, is_bfloat16_supported
import random, math, torch
from torch.cuda.amp import autocast

# 1) Build your TrainingArguments ONCE, with remove_unused_columns=False
training_args = TrainingArguments(
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=5,
    num_train_epochs=1,
    max_steps=-1,
    learning_rate=2e-4,
    fp16=not is_bfloat16_supported(),
    bf16=is_bfloat16_supported(),
    logging_steps=1,
    eval_strategy="steps",
    eval_steps=40,
    per_device_eval_batch_size=2,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    remove_unused_columns=False,
    seed=3407,
    output_dir="outputs",
    report_to="none",
)

# tokenize
def tokenize_fn(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="longest",
    )

train_dataset = train_dataset.map(
    tokenize_fn,
    batched=True,
    remove_columns=["text"],
)
validation_dataset = validation_dataset.map(
    tokenize_fn,
    batched=True,
    remove_columns=["text"],
)
test_dataset = test_dataset.map(
    tokenize_fn,
    batched=True,
    remove_columns=["text"],
)

# 2) Instantiate *once*
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    dataset_text_field="text",
    packing=True,
    args=training_args,
)

# initial perplexity
import math
full_metrics = trainer.evaluate(eval_dataset=test_dataset)
print("200-sample Perplexity on test:", math.exp(full_metrics["eval_loss"]))

# @title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

import re

# Prepare a list to collect all generated responses

FastLanguageModel.for_inference(model)  # enable optimized inference

for i, sample in enumerate(hf_set):
    # 1) Print the original formatted human‐feedback sample
    sample_text = sample["text"]

    # Split on the three markers; the regex will drop them for us
    parts = re.split(
        r"\r?\n\r?\n### Instruction:\r?\n|\r?\n\r?\n### Input:\r?\n|\r?\n\r?\n### Response:\r?\n",
        sample_text
    )

    # re.split returns ['', '<instr>', '<inp>', '<resp>'], so skip the first empty
    _, instruction, input, reference = parts

    # strip any stray whitespace
    instruction = instruction.strip()
    input = input.strip()
    reference = reference.strip()

    FastLanguageModel.for_inference(model) # Enable native 2x faster inference
    inputs = tokenizer(
    [
        alpaca_prompt.format(
            instruction, # Instruction
            input, # Input
            "", # output - leave this blank for generation!
        )
    ], return_tensors = "pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens = 500, use_cache = True)
    prompt = tokenizer.batch_decode(outputs)[0]

    full = (
        prompt  + "\n\n"
        "### Reference:\n"   + reference
    )
    print(f"\nHuman Feedback sample {i}:\n{full}")

trainer.train()

import math
full_metrics = trainer.evaluate(eval_dataset=test_dataset)
print("200-sample Perplexity:", math.exp(full_metrics["eval_loss"]))

import re

# Prepare a list to collect all generated responses

FastLanguageModel.for_inference(model)  # enable optimized inference

for i, sample in enumerate(hf_set):
    # 1) Print the original formatted human‐feedback sample
    sample_text = sample["text"]
    # assume sample_text is your full Alpaca‐style string
    # e.g. "…\n\n### Instruction:\n<instr>\n\n### Input:\n<inp>\n\n### Response:\n<resp><|end_of_text|>"

    # Split on the three markers; the regex will drop them for us
    parts = re.split(
        r"\r?\n\r?\n### Instruction:\r?\n|\r?\n\r?\n### Input:\r?\n|\r?\n\r?\n### Response:\r?\n",
        sample_text
    )

    # re.split returns ['', '<instr>', '<inp>', '<resp>'], so skip the first empty
    _, instruction, input, reference = parts

    # strip any stray whitespace
    instruction = instruction.strip()
    input = input.strip()
    reference = reference.strip()

    FastLanguageModel.for_inference(model) # Enable native 2x faster inference
    inputs = tokenizer(
    [
        alpaca_prompt.format(
            instruction, # Instruction
            input, # Input
            "", # output - leave this blank for generation!
        )
    ], return_tensors = "pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens = 500, use_cache = True)
    prompt = tokenizer.batch_decode(outputs)[0]

    # 2) Build the generation prompt by keeping everything up to "### Response:"
    #prompt = "### Response:\n" + sample_text.split("### Response:")[1]

    # 5) Save and print the generated reply
    # now concatenate strings
    full = (
        prompt  + "\n\n"
        "### Reference:\n"   + reference
    )
    print(f"\nHuman Feedback sample {i}:\n{full}")

# @title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")